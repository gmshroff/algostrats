{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import import_ipynb\n",
    "import random\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Net,X_test,y_test,verbose=True):\n",
    "    Net.eval()\n",
    "    m = X_test.shape[0]\n",
    "    y_pred = Net(X_test)\n",
    "    predicted = torch.max(y_pred, 1)[1]\n",
    "    correct = (predicted == y_test).float().sum().item()\n",
    "    if verbose: print(correct,m)\n",
    "    accuracy = correct/m\n",
    "    Net.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(Net,data,epochs=20,lr=None,Loss=nn.NLLLoss(),verbose=False):\n",
    "    if lr!=None: Net.optimizer = optim.Adam(Net.parameters(),lr=lr)\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for e in range(epochs):\n",
    "        step=0\n",
    "        tot_loss=0.0\n",
    "        acc=0.0\n",
    "        b=0\n",
    "        for (X,y) in data:\n",
    "            y_pred = Net(X)\n",
    "            loss = Loss(y_pred,y)\n",
    "            Net.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            Net.optimizer.step()\n",
    "            step+=1\n",
    "            tot_loss+=loss\n",
    "            acc+=accuracy(Net,X,y,verbose=False)\n",
    "            b+=1\n",
    "        l = tot_loss.item()/step\n",
    "        a = acc/step\n",
    "        losses += [l]\n",
    "        accs += [a]\n",
    "        if verbose: \n",
    "            print('Epoch  % 2d Loss: %2.5e Accuracy: %2.5f'%(e,l,a))\n",
    "            display.clear_output(wait=True)\n",
    "    return Net,losses,accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-layer perceptron with ReLU non-lineartities; for classification or regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,dims=[5,3,2],task='classification',lr=1e-3):\n",
    "        super(MLP,self).__init__()\n",
    "        self.dims=dims\n",
    "        self.n = len(self.dims)-1\n",
    "        self.task=task\n",
    "        self.layers=nn.ModuleList()\n",
    "        for i in range(self.n-1):\n",
    "            self.layers.append(nn.Linear(dims[i],dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        if task=='classification': \n",
    "            self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "            self.layers.append(nn.LogSoftmax(dim=1))\n",
    "        elif task=='regression': \n",
    "            self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "            self.layers.append(nn.Linear(dims[i+2],1))\n",
    "        else: self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self,x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,lr):\n",
    "        # This just calls the base class constructor\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        # Neural network layers assigned as attributes of a Module subclass\n",
    "        # have their parameters registered for training automatically.\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.logsoft = nn.LogSoftmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self, x):\n",
    "        # The RNN also returns its hidden state but we don't use it.\n",
    "        # While the RNN can also take a hidden state as input, the RNN\n",
    "        # gets passed a hidden state initialized with zeros by default.\n",
    "        if self.input_size==1:x=x.unsqueeze(-1)\n",
    "        h = self.rnn(x)[0]\n",
    "        x = self.linear(h)\n",
    "        x = self.logsoft(x)\n",
    "        x = x[:,-1,:]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizeL, output_size, lr):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_sizeL[0], batch_first=True)\n",
    "        self.linear1 = torch.nn.Linear(hidden_sizeL[0], hidden_sizeL[1])\n",
    "        self.linear2 = torch.nn.Linear(hidden_sizeL[1], output_size)\n",
    "        self.logsoft = nn.LogSoftmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        self.dropout = nn.Dropout(.3)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        if self.input_size==1:x=x.unsqueeze(-1)\n",
    "        h = self.lstm(x)[0]\n",
    "        x = h[:,-1,:]\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.logsoft(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
