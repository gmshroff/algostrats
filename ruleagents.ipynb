{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7889ded-dd8c-4c73-aadd-687cf98ddbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from datetime import datetime as dt\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd63887a-5eb4-4b8e-a915-47a864cc08f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from aiagentbase.ipynb\n"
     ]
    }
   ],
   "source": [
    "from aiagentbase import AIAgent,Controller,Memory,Perception,Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afea58b9-6c56-40c8-a101-32cba908483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OHLCV_COLS=['Open_n','High_n','Low_n','Close_n','Volume_n']\n",
    "TA_COLS=['SMA_10', 'SMA_20','VOL_SMA_20','RSI_14','BBL_5_2.0','BBM_5_2.0','BBU_5_2.0',\n",
    "       'BBB_5_2.0', 'BBP_5_2.0','MACD_12_26_9','MACDh_12_26_9','MACDs_12_26_9','VWAP_D',\n",
    "        'MOM_30','CMO_14']\n",
    "COLS=OHLCV_COLS+TA_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f49cf9-1c4f-43db-b236-9cf5d55b6e5b",
   "metadata": {},
   "source": [
    "### Rule-based strategies as agents\n",
    "See ruleagents_dev: tested with backtest. Also with tradeserver. TBD: common rewards format\n",
    "Note: only works for local strategies (not remote: TBD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "358be953-26c6-4e1a-ad66-1a30a5db52f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleAgent(AIAgent):\n",
    "    def __init__(self):\n",
    "        self.agent=True\n",
    "        self.tidx=0\n",
    "        self.owner=None\n",
    "        super().__init__()\n",
    "        # self.memory=SplMemory()\n",
    "        # self.memory.parent=self\n",
    "        self.use_memory=False\n",
    "        ## Augmentations of AIAgent\n",
    "        self.actor=self.Actor(parent=self)\n",
    "        self.perception=self.Perception(parent=self)\n",
    "        self.memory=self.Memory(parent=self)\n",
    "        # self.actor.call_model=self.call_model\n",
    "        # self.perception.perceive_state=self.perceive_state\n",
    "        # self.perception.perceive_reward=self.perceive_reward\n",
    "        # self.perception.action_perceptual=self.action_perceptual\n",
    "        # self.actor.percept_to_state=self.percept_to_state\n",
    "        # self.actor.compute_reward=self.compute_reward\n",
    "        self.logL=[]\n",
    "    ##Interface to tradeserver\n",
    "    def set_alt_data(self,alt_data_func,remote=False):\n",
    "        if remote: self.gdata=anvil.server.call(alt_data_func)['gdata']\n",
    "        else: self.gdata=alt_data_func()['gdata']\n",
    "    def act_on_entry(self):\n",
    "        if self.owner==None: return True\n",
    "        elif self.owner.status[self.scantickers[self.tidx]]=='deployed': return False\n",
    "        else: return True\n",
    "    def act_on_exit(self):\n",
    "        if self.owner==None: return True\n",
    "        elif self.owner.status[self.scantickers[self.tidx]]=='active': return False\n",
    "        else: return True\n",
    "    def check_entry_batch(self,dfD):\n",
    "        if self.act_on_entry(): return self.act(('entry',dfD))\n",
    "        else: return self.check_entry(dfD)\n",
    "    def save_func(self,episode_state):\n",
    "        return ()\n",
    "    def check_exit_batch(self,dfD,posf):\n",
    "        def exit_fn(row):\n",
    "            if self.act_on_exit() and row.ticker==self.scantickers[self.tidx]:\n",
    "                return self.act(('exit',row,dfD[row.ticker]))\n",
    "            else: return self.exit_func(row,dfD[row.ticker])\n",
    "        posf['to_exit']=posf.apply(exit_fn,axis=1).values\n",
    "        return posf\n",
    "    def exit_predicate(self,row,df):\n",
    "        return self.act(('exit',row,df))\n",
    "    def Check(strat,dfD):\n",
    "        return strat.check_entry_batch(dfD)\n",
    "    def Exit(strat,dfD,posf):\n",
    "        return strat.check_exit_batch(dfD,posf)\n",
    "    ## Augmentations of AIAgent for trade sim world\n",
    "    def call_model(self,state):\n",
    "        ##Overriding AIAgent.Model\n",
    "        #override with actual policy in subclass\n",
    "        return None\n",
    "    def compute_state(self,percept):\n",
    "        trt=self.scantickers[self.tidx]\n",
    "        self.logL+=[percept,trt]\n",
    "        if percept[0]=='entry':\n",
    "            state=torch.tensor(percept[1][trt][COLS].values)\n",
    "        elif percept[0]=='exit':\n",
    "            state=torch.tensor(percept[2][COLS].values)\n",
    "        return state\n",
    "    def clear_sar_memory(self):\n",
    "        pass\n",
    "    \n",
    "    class Perception(Perception):\n",
    "        def __init__(self,parent): \n",
    "            super().__init__(parent=parent)\n",
    "        def perceive_state(self,world_state):\n",
    "            return {'world_state':world_state,'ticker':self.parent.scantickers[self.parent.tidx]}\n",
    "        def perceive_reward(self,reward):\n",
    "            #Override AIAgent\n",
    "            actor_reward=reward\n",
    "            return actor_reward,{'ticker':self.parent.scantickers[self.parent.tidx]}\n",
    "        def action_perceptual(self,action):\n",
    "            trt=self.parent.scantickers[self.parent.tidx]\n",
    "            if type(action)==tuple: action_to_store=action[0][trt]\n",
    "            elif type(action)==bool: action_to_store=int(action)\n",
    "            else: action_to_store=type(action)\n",
    "            return action_to_store\n",
    "\n",
    "    class Actor(Actor):\n",
    "        def __init__(self,parent): \n",
    "            super().__init__(parent=parent)\n",
    "        def percept_to_state(self,perceived_state):\n",
    "            return perceived_state['world_state']\n",
    "        def compute_reward(self,reward):\n",
    "            if type(reward[0])==list:\n",
    "                if len(reward[0])==0: return 0\n",
    "                else:\n",
    "                    return reward[0][0]['ppnl']\n",
    "            elif type(reward[0])==tuple: return reward[0][0]\n",
    "            elif reward is None: return 0\n",
    "            else: return type(reward)\n",
    "        def call_model(self,state):\n",
    "            return self.parent.call_model(state)\n",
    "        \n",
    "    class Memory(Memory):\n",
    "        def __init__(self,parent):\n",
    "            super().__init__(parent=parent)\n",
    "        def add_state_action(self,actor_state,action,time):\n",
    "            actor_state=self.parent.compute_state(actor_state)\n",
    "            super().add_state_action(actor_state,action,time)\n",
    "        def update_next_state(self,actor_state,time):\n",
    "            if time not in self.sar_memory: self.sar_memory[time]={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72360c93-d90b-4bdf-8093-fd7c914f010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplMemory(Memory):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def add_state_action(self,actor_state,action,time):\n",
    "        actor_state=self.parent.compute_state(actor_state)\n",
    "        super().add_state_action(actor_state,action,time)\n",
    "    def update_next_state(self,actor_state,time):\n",
    "        if time not in self.sar_memory: self.sar_memory[time]={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6c1a4-6777-49b5-b0a0-0703db78cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaMomCMOAgent(RuleAgent):\n",
    "    # Adaptive momentum strategy using CMO and ADF for non-stationarity\n",
    "    def __init__(self,high=80,low=20,mid=50):\n",
    "        super(AdaMomCMOAgent,self).__init__()\n",
    "        self.logL=[]\n",
    "        self.high=high\n",
    "        self.low=low\n",
    "        self.mid=mid\n",
    "        self.model_type='rule_based'\n",
    "        self.data_cols=['datetime']+COLS\n",
    "        self.regime={}\n",
    "        self.entry_val={}\n",
    "        self.exit_val={}\n",
    "        self.logL=[]\n",
    "        self.rewL=[]\n",
    "    def call_model(self,state):\n",
    "        # super().act(state)\n",
    "        if state[0]=='entry': return self.check_entry(state[1])\n",
    "        elif state[0]=='exit': return self.exit_func(state[1],state[2])\n",
    "    def check_entry(self,dfD):\n",
    "        timenow=[dfD[t].iloc[-1]['datetime'] for t in dfD][0]\n",
    "        hour,minute=timenow.hour,timenow.minute\n",
    "        decisionsD={t:0 for t in dfD}\n",
    "        stopD={t:5 for t in dfD}\n",
    "        targetD={t:5 for t in dfD}\n",
    "        if hour==9 and minute<=35: return decisionsD,stopD,targetD\n",
    "        dataD={}\n",
    "        log_entry={}\n",
    "        high=self.high\n",
    "        low=self.low\n",
    "        mid=self.mid\n",
    "        for t in dfD.keys():\n",
    "            data=dfD[t]\n",
    "            row=dfD[t].iloc[-1]\n",
    "            if data.shape[0]>=65:\n",
    "                # self.logL+=[(t,timenow,data['Close_n'])]\n",
    "                adf=adfuller(data['Close_n'],maxlag=30,autolag=None)\n",
    "                if adf[0]>adf[4]['1%']: self.regime[t]='tr'\n",
    "                else: self.regime[t]='mr'\n",
    "                regime=self.regime[t]\n",
    "                if regime=='tr' and row['CMO_14']>low and row['CMO_14']<mid: decisionsD[t]=1\n",
    "                elif regime=='tr' and row['CMO_14']<-low and row['CMO_14']>-mid: decisionsD[t]=-1\n",
    "                elif regime=='mr' and row['CMO_14']>high: decisionsD[t]=-1\n",
    "                elif regime=='mr' and row['CMO_14']<-high: decisionsD[t]=1\n",
    "                else: decisionsD[t]=0\n",
    "                self.entry_val[t]=row['CMO_14']\n",
    "                self.exit_val[t]='not_set'\n",
    "        # return always_buy(dfD)\n",
    "        return decisionsD,stopD,targetD\n",
    "    def exit_func(self,row,df):\n",
    "        # return True\n",
    "        data=df\n",
    "        dfrow=df.iloc[-1]\n",
    "        high=self.high\n",
    "        low=self.low\n",
    "        mid=self.mid\n",
    "        regime=self.regime[row['ticker']]\n",
    "        self.exit_val[row['ticker']]=dfrow['CMO_14']\n",
    "        if regime=='tr' and row['quant']>0 and dfrow['CMO_14']>high: return True\n",
    "        elif regime=='tr' and row['quant']<0 and dfrow['CMO_14']<-high: return True\n",
    "        elif regime=='mr' and row['quant']>0 and dfrow['CMO_14']>-low: return True\n",
    "        elif regime=='mr' and row['quant']<0 and dfrow['CMO_14']<low: return True\n",
    "        # exit cases for detecting a trade on incorrect trend direction\n",
    "        # elif regime=='tr' and row['quant']>0 and dfrow['CMO_14']<=-mid: return True\n",
    "        # elif regime=='tr' and row['quant']<0 and dfrow['CMO_14']>=mid: return True\n",
    "        else: return False\n",
    "    def save_func(self,episode_state):\n",
    "        ticker=[t for t in episode_state][0]\n",
    "        return ticker,self.entry_val[ticker],self.exit_val[ticker]\n",
    "    def reward(self,reward):\n",
    "        self.rewL+=[reward]\n",
    "        super().reward(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae9674-33d8-4b7f-9516-cdc087353aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GapBetAgent(RuleAgent):\n",
    "    def __init__(self,entry=50,exit=75,direction=-1):\n",
    "        super(GapBetAgent,self).__init__()\n",
    "        self.entry=entry\n",
    "        self.exit=exit\n",
    "        self.data_cols=['CMO_14','datetime']\n",
    "        self.model_type='rule-based'\n",
    "        # self.direction=direction\n",
    "    def act(self,state):\n",
    "        if state[0]=='entry': return self.check_entry(state[1])\n",
    "        elif state[0]=='exit': return self.exit_func(state[1],state[2])\n",
    "    def check_entry(self,dfD):\n",
    "        decisionsD={t:0 for t in dfD}\n",
    "        stopD={t:0.25 for t in dfD}\n",
    "        targetD={t:2 for t in dfD}\n",
    "        timenow=[dfD[t].iloc[-1]['datetime'] for t in dfD][0]\n",
    "        date=timenow.strftime('%d-%b-%Y')\n",
    "        gdir=global_direction(self.gdata[date][0])\n",
    "        if abs(gdir)>.5: self.direction=1\n",
    "        else: self.direction=-1\n",
    "        hour,minute=timenow.hour,timenow.minute\n",
    "        if hour>9 or (hour==9 and minute>35): return decisionsD,stopD,targetD\n",
    "        for t in dfD:\n",
    "            row=dfD[t].iloc[-1]\n",
    "            if row['CMO_14']>self.entry: decisionsD[t]=self.direction\n",
    "            elif row['CMO_14']<-self.entry: decisionsD[t]=-self.direction\n",
    "        return decisionsD,stopD,targetD\n",
    "    def exit_func(self,row,posf):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e90844-5b84-45c2-a37a-9d8741cc98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing(dfD):\n",
    "    empty={t:0 for t in dfD}\n",
    "    return empty,empty,empty\n",
    "def always_buy(dfD):\n",
    "    buy={t:1 for t in dfD}\n",
    "    empty={t:0 for t in dfD}\n",
    "    return buy,empty,empty\n",
    "def always_sell(dfD):\n",
    "    sell={t:-1 for t in dfD}\n",
    "    empty={t:0 for t in dfD}\n",
    "    return sell,empty,empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563a185-9b15-4bd5-8cfa-9250a7e2db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_direction(gdata):\n",
    "    global_tickers=['^NYA','LSEG.L','^IXIC']\n",
    "    direction={}\n",
    "    for g in global_tickers:\n",
    "        direction[g]=gdata['Close_'+g]-gdata['Open_'+g]\n",
    "    return 100*sum([direction[k] for k in direction])/len(direction)\n",
    "def domestic_direction(gdata):\n",
    "    tickers=['^NSEI']\n",
    "    direction={}\n",
    "    for g in tickers:\n",
    "        direction[g]=gdata['Close_'+g]-gdata['Open_'+g]\n",
    "    return 100*sum([direction[k] for k in direction])/len(direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783c014-70c7-49eb-959e-9dc18468abf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
