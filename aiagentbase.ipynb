{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad623b-69c2-4eb3-8bcf-1e571030186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from numpy.distutils.misc_util import is_sequence\n",
    "import threading\n",
    "# from stable_baselines3 import PPO,DQN,A2C,SAC\n",
    "import gym\n",
    "from gym import spaces,Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472906b-6160-4c43-b24a-e3078b60be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.monitor import Monitor as Mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c10260-6176-4aa0-8915-d4676cf899f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a952b6-ba67-4958-a669-ed86af67e676",
   "metadata": {},
   "source": [
    "### Base classes for an **AI Agent** having the following architecture:\n",
    "1.1 An **AI Agent** operates in a '**World**' that calls it via '**action<-act(state)**' requests and '**reward(reward)**' intimations. (Worlds are typically wrappers around traditional RL-environments; Worlds can also wrap supervised learning tasks.) The goal of the AI Agent is to maximise *long-term steady-state average reward*. Periodically the World may also update the Agent regarding the completion of an *episode* (e.g. an RL episode or completion of epoch).\n",
    "\n",
    "1.2 An Agent has **Controller**, **Perception**, **Memory** and **Actor** components (In line with Lecun's \"Archicture of an Autonomous AI Agent\". World Model to be added later.) The Memory contains a Perceptual Memory as well as a State-Action-Reward memory. The Actor includes a **Model**. A Model includes a **Network** and **Trainer** (class that handles publishes training procedure(s) for the Network). Overall orchestration of all components including the Agent's public interface is handled by the Controller. Further, the learning schedule, to train the Network, be it done only intitally, periodically, or continually online, is decided by the Controller. \n",
    "\n",
    "1.3 Each component of an Agent may be customised for a specific World by inheriting from the default base class for that component. Agent's *begin* method indicates that a new episode/epoch is starting and resets/increments the time/ep counters (see below); the *clear* method clears (e.g. removes all storage) from applicable components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698e36b-02a9-4850-99d5-ecf757b4558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIAgent():\n",
    "    def __init__(self,controller=None,perception=None,memory=None,actor=None):\n",
    "        if controller is not None: self.controller=controller\n",
    "        else: self.controller=Controller(parent=self)\n",
    "        if perception is not None: self.perception=perception\n",
    "        else: self.perception=Perception(parent=self)\n",
    "        if memory is not None: self.memory=memory\n",
    "        else: self.memory=Memory(parent=self)\n",
    "        if actor is not None: self.actor=actor\n",
    "        else: self.actor=Actor(parent=self)\n",
    "        self.time=0\n",
    "        self.ep=[]\n",
    "        # self.controller.parent=self\n",
    "        # self.perception.parent=self\n",
    "        # self.memory.parent=self\n",
    "        # self.actor.parent=self\n",
    "        self.debug=False\n",
    "        self.use_memory=True\n",
    "        self.limit_memory=False\n",
    "    def act(self,world_state):\n",
    "        world_action = self.controller.act(world_state)\n",
    "        # check to see if network needs training - TBDesigned\n",
    "        self.time+=1\n",
    "        return world_action\n",
    "    def reward(self,world_reward):\n",
    "        return self.controller.reward(world_reward)\n",
    "    def episode(self):\n",
    "        return self.controller.episode()\n",
    "    def begin(self,state=None):\n",
    "        ## TBD may need to do more - both episode and time may be needed and episode reset\n",
    "        self.ep+=[self.time]\n",
    "    def clear(self):\n",
    "        self.memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf59ae-df32-4555-b916-0df080f8a5a7",
   "metadata": {},
   "source": [
    "2.1 Control flow goes as follows: Agent tracks a *time* counter and a list *ep*; the latter tracks episodes/epochs and is set to the first time counter for each epoch/episode e.g. ep[e]=starting time of epoch/epsode e. World calls *act*(world-state) on Agent, which is routed to Controller's *act*. The incoming *world-state* is mapped to a percept using the *perceive_state* function published by the Perception module, and stored in the perceptual memory (via Memory's *add_percept*, against the current *time*). \n",
    "\n",
    "2.2 The Actor reads from the perceptual memory and creates an actor-state by processing the current percept (and possibly also using prior rewards and actions, e.g. for meta-RL). The Actor also updates the state-action-reward memory with the previous time's percept after mapping it to an actor-state.\n",
    "\n",
    "2.3 The Actor calls its Model to decide the action to return. Before returning the action, it is stored in the perceptual memory; a new entry is also created in the state-action-reward memory with the current actor-state and action. Also, the action is mapped to a world_action using the Perception component's *action_to_world* function.\n",
    "\n",
    "2.4 Before completing the *act* (or *reward*) flow, Actor checks to see if any periodic or online training is needed, and updates the *training* flag accordingly. It also updates the *time* counter.\n",
    "\n",
    "2.5 On receiving a *world_reward* from the World, the Actor passes it to the Controller that extracts information using Perception's *perceive_reward* function.  These are stored in the perceptual memory (for the prior time step, since by now the Actor's time step has been update as soon as its action was completed) as well as appended to the latest entry (prev time step) of the state-action-reward memory. Note: e.g. additional information might include, in addition to state, labels in case of a supervised learning scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209cb81d-65f1-470d-bf88-72cfe43af453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller():\n",
    "    def __init__(self,parent=None):\n",
    "        self.parent=parent\n",
    "    def act(self,world_state):\n",
    "        perceived_state=self.parent.perception.perceive_state(world_state)\n",
    "        if self.parent.debug: print(perceived_state)\n",
    "        if self.parent.use_memory: self.parent.memory.add_percept(perceived_state,self.parent.time)\n",
    "        actor_state=self.parent.actor.percept_to_state(perceived_state)\n",
    "        if self.parent.use_memory: actor_state=self.parent.actor.augment_state(actor_state)\n",
    "        if self.parent.debug: print('actor_state:',actor_state)\n",
    "        if self.parent.use_memory: self.parent.memory.update_next_state(actor_state,self.parent.time-1)\n",
    "        action=self.parent.actor.call_model(actor_state)\n",
    "        if self.parent.debug: print('action:',action)\n",
    "        if self.parent.use_memory: action_to_store=self.parent.perception.action_perceptual(action)\n",
    "        if self.parent.use_memory: \n",
    "            intrinsic_reward=self.parent.actor.intrinsic_reward(perceived_state,action_to_store,actor_state)\n",
    "        # if self.parent.use_memory: self.parent.memory.update_reward_sar(intrinsic_reward,self.parent.time-1)\n",
    "        if self.parent.use_memory: self.parent.memory.update_reward_sar(intrinsic_reward,self.parent.time)\n",
    "        if self.parent.use_memory: self.parent.memory.update_action_perceptual(action_to_store,self.parent.time)\n",
    "        if self.parent.use_memory: self.parent.memory.add_state_action(actor_state,action_to_store,self.parent.time)\n",
    "        world_action=self.parent.perception.action_to_world(action)\n",
    "        if self.parent.limit_memory: self.parent.memory.pop()\n",
    "        return world_action\n",
    "    def reward(self,world_reward):\n",
    "        reward=self.parent.perception.perceive_reward(world_reward)\n",
    "        if self.parent.use_memory: self.parent.memory.update_reward_perceptual(reward,self.parent.time-1)\n",
    "        if self.parent.use_memory: reward_sar=self.parent.actor.compute_reward(reward)\n",
    "        if self.parent.use_memory: self.parent.memory.update_reward_sar(reward_sar,self.parent.time-1)\n",
    "        if self.parent.limit_memory: self.parent.memory.pop() \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29715c63-7fc0-4941-9fa5-30cdaf02543b",
   "metadata": {},
   "source": [
    "3.1 ***Memory*** stores are nested dictionaries indexed by *time*. Each entry is a dictionary with keys *typically* being *'percept','action','reward'* and *'state','action','reward','next_state'* for perceptual memory / state-action-reward memory respectively. However these may be extended for specifc kinds of worlds, e.g. supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28661473-fc0b-470c-93cd-2cc08c45c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self,parent=None):\n",
    "        self.parent=parent\n",
    "        self.clear()\n",
    "        self.limit_perceptual=None\n",
    "        self.limit_sar=None\n",
    "    def pop(self):\n",
    "        psize=self.limit_perceptual\n",
    "        M=self.perceptual_memory\n",
    "        if psize!=None: \n",
    "            for t in [t for t in M if t<self.parent.time-psize]: M.pop(t)\n",
    "        S=self.sar_memory\n",
    "        ssize=self.limit_sar\n",
    "        if ssize!=None: \n",
    "            for t in [t for t in S if t<self.parent.time-ssize]: S.pop(t)\n",
    "    def clear(self):\n",
    "        self.perceptual_memory={}\n",
    "        self.sar_memory={}\n",
    "    def add_percept(self,perceived_state,time):\n",
    "        self.perceptual_memory[time]=perceived_state\n",
    "        if self.parent.debug: print('add_percept:',self.perceptual_memory)\n",
    "    def update_next_state(self,actor_state,time):\n",
    "        if time in self.sar_memory: self.sar_memory[time]['next_state']=actor_state\n",
    "        else: self.sar_memory[time]={'next_state':actor_state}\n",
    "        if self.parent.debug: print('update_next_state:',self.sar_memory)\n",
    "    def update_action_perceptual(self,action,time):\n",
    "        self.perceptual_memory[time]['action']=action\n",
    "        if self.parent.debug: print('update_action_perceptual:',self.perceptual_memory)\n",
    "    def add_state_action(self,actor_state,action,time):\n",
    "        if time in self.sar_memory: \n",
    "            self.sar_memory[time]['state']=actor_state\n",
    "            self.sar_memory[time]['action']=action\n",
    "        else: self.sar_memory[time]={'state':actor_state,'action':action}\n",
    "        if self.parent.debug: print('add_state_action:',self.sar_memory)\n",
    "    def update_reward_perceptual(self,reward,time):\n",
    "        self.perceptual_memory[time]['reward']=reward\n",
    "        if self.parent.debug: print('update_reward_perceptual:',self.perceptual_memory)\n",
    "    def update_reward_sar(self,reward,time):\n",
    "        if time in self.sar_memory:\n",
    "            if 'reward' in self.sar_memory[time]:self.sar_memory[time]['reward']+=reward\n",
    "            else: self.sar_memory[time]['reward']=reward\n",
    "        else:\n",
    "            self.sar_memory[time]={'reward':reward}\n",
    "        if self.parent.debug: print('update_reward_sar:',self.sar_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba530e0a-0173-4319-a133-0616b0d08eae",
   "metadata": {},
   "source": [
    "3.2 The default **Perception** class just copies world states/actions/rewards to actor states/actions/rewards. Should be subclassed for a given World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e870706-16c2-491c-9fbd-fefd9bd3a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perception():\n",
    "    def __init__(self,parent=None):\n",
    "        self.parent=parent\n",
    "    def perceive_state(self,world_state):\n",
    "        if type(world_state)==dict: return world_state\n",
    "        else: return {'percept':world_state}\n",
    "    def action_to_world(self,action):\n",
    "        return action\n",
    "    def action_perceptual(self,action):\n",
    "        return action\n",
    "    def perceive_reward(self,reward):\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e3ecd-7499-4bdb-b268-4c3a9264a57c",
   "metadata": {},
   "source": [
    "3.3 The default **Actor** has no Model and returns a fixed action (can be set). It copies the percept from perceptual memory directly into the actor_state. This should be subclassed and/or method *percept_to_state* or *create_actor_state* overridden for a given World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d7d18-d6ab-4a72-b83e-d73b4c541596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    def __init__(self,parent=None,model=None):\n",
    "        self.parent=parent\n",
    "        self.model=model\n",
    "        self.default_action='default_action'\n",
    "        self.intrinsic_reward_value=0\n",
    "    def percept_to_state(self,perceived_state):\n",
    "        return perceived_state['percept']\n",
    "    def augment_state(self,state):\n",
    "        return state\n",
    "    def call_model(self,actor_state):\n",
    "        return self.default_action\n",
    "    def compute_reward(self,reward):\n",
    "        return reward\n",
    "    def intrinsic_reward(self,precept,action,state):\n",
    "        return self.intrinsic_reward_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd2066b-e0f1-42ca-a7fb-3301fa4e72d2",
   "metadata": {},
   "source": [
    "### Guidelines on overriding/augmenting base classes for world-specific agents\n",
    "Template indicating methods that need to be overridden/augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf26c60-2c88-43f3-bf05-ee74bbc3017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateAIAgent(AIAgent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ##Augmenting AIAgent\n",
    "        self.actor=self.Actor(parent=self)\n",
    "        # etc. for all augmented/overridden components\n",
    "    def reward(self,reward):\n",
    "        ##Augmenting AIAgent\n",
    "        return super().reward(reward)\n",
    "    def begin(self,state):\n",
    "        ##Augmenting AIAgent\n",
    "        super().begin(state)\n",
    "        \n",
    "    class Actor(Actor):\n",
    "        def __init__(self,parent):\n",
    "            super().__init__(parent=parent)\n",
    "        def call_model(self,state):\n",
    "            ##Overriding AIAgent\n",
    "            action=self.action_space.sample() #override with actual policy\n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a833f9-5415-40a2-b216-b5df6aa040e7",
   "metadata": {},
   "source": [
    "### Generic (On-policy) Reinforcement Learning Agent including Windowing and Meta-RL\n",
    "Create a local environment called by a Monitor thread running within Agent, which implements or\n",
    "re-uses an on-policy RL training procedure (such as PPO etc.). The env has an input and output queue. If a training flag is set, the Agent uses a ProxyModel place of its normal model to compute actions: The actor-state is placed in the env's input queue and an action is awaited from the env's output queue.\n",
    "\n",
    "The monitor starts by calling the env.reset method that waits on the input queue to receive \n",
    "and then return a state. The monitor thread computes an action on the current state and calls\n",
    "env.step(action), which places the action in the output queue and awaits a reward from the input\n",
    "queue. After receiving a reward, step again waits for the next state on the input queue.\n",
    "Once this is also received, both next stte and reward are returned.\n",
    "\n",
    "The generic RL agent incorporates windowing, i.e., past win states are concatenated, and meta-RL via the $RL^2$ algorithm (metarl parameter). Note that if win>1 then use_memory has to be true as the window computation uses memory. Further metarl=True requires win>=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67561fa1-4d6f-4e0a-b11f-71dbf3d2233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAgent(AIAgent):\n",
    "    # RL Agent using window of previous states taken from memory: parameter win - length\n",
    "    def __init__(self,algoclass,monclass,action_space,observation_space,policytype='MlpPolicy',\n",
    "                n_steps=2048,verbose=1,win=1,soclass=None,metarl=False):\n",
    "        ##Augmenting AIAgent\n",
    "        super().__init__()\n",
    "        ##Local RL environment running in a thread to interact with World via queues\n",
    "        self.env=self.TrainingEnv(parent=self)\n",
    "        ##Monitoring (need to move class to input parameter and cascaded for scripts to work)\n",
    "        self.env=monclass(self.env,'/tmp/aiagents')\n",
    "        self.env.action_space=action_space\n",
    "        self.action_dim=(lambda a: 1 if len(a.shape)==0 else a.nvec.flatten().shape[0])(action_space)\n",
    "        if metarl: observation_space=self.expand_obs_space(observation_space,action_space)\n",
    "        if win>1:self.env.observation_space=soclass(1,win,observation_space).stack_observation_space(observation_space)\n",
    "        else:self.env.observation_space=observation_space\n",
    "        self.monitor=self.Monitor(parent=self)\n",
    "        self.set_training(True)\n",
    "        self.tot_rew=0\n",
    "        self.logL=[]\n",
    "        self.kill=False\n",
    "        ##Override Actor\n",
    "        self.model=algoclass(policytype, self.env, verbose=verbose,n_steps=n_steps)\n",
    "        self.actor=self.Actor(parent=self,model=self.model)\n",
    "        self.perception.perceive_reward=self.perceive_reward\n",
    "        ## Win/MetaRL Specific\n",
    "        self.win=win\n",
    "        self.metarl=metarl\n",
    "        self.use_memory=True\n",
    "        self.monitorname='monitor'\n",
    "    \n",
    "    def expand_obs_space(self,obs_space,action_space):\n",
    "        if len(action_space.shape)==0:\n",
    "            high=np.concatenate([obs_space.high,np.array([action_space.n,np.inf])])\n",
    "            low=np.concatenate([obs_space.low,np.array([0,-np.inf])])\n",
    "        else:\n",
    "            h=action_space.nvec.flatten()\n",
    "            z=np.zeros(action_space.nvec.flatten().shape[0])\n",
    "            high=np.concatenate([obs_space.high,h,np.array([np.inf])])\n",
    "            low=np.concatenate([obs_space.low,z,np.array([-np.inf])])\n",
    "        obs_space.high=high\n",
    "        obs_space.low=low\n",
    "        return obs_space\n",
    "    \n",
    "    def start(self,training_steps=20000):\n",
    "        # self.monitorthread=Thread(name='monitor',target=self.monitor.run,args=(training_steps,))\n",
    "        self.monitorthread=Thread(name=self.monitorname,target=self.monitor.train,args=(training_steps,))\n",
    "        self.monitorthread.start() \n",
    "    \n",
    "    def stop(self):\n",
    "        self.kill=True\n",
    "    \n",
    "    def log(self,entry):\n",
    "        self.logL+=[entry]\n",
    "        \n",
    "    def set_training(self,value):\n",
    "        self.training=value\n",
    "        \n",
    "    class TrainingEnv(Env):\n",
    "        def __init__(self,parent):\n",
    "            self.parent=parent\n",
    "            self.inputS=Queue() #written by act read by reset and step\n",
    "            self.outputS=Queue() #written by act read by act\n",
    "            self.rewardI=Queue() #written by act and read by step\n",
    "            self.actionO=Queue() #written by step and read by act\n",
    "            self.counter=0\n",
    "        def reset(self):\n",
    "            # print('reset')\n",
    "            self.parent.log(('reset waiting',self.counter))\n",
    "            self.state=self.inputS.get()\n",
    "            return self.state\n",
    "        def step(self,action):\n",
    "            if self.parent.kill: sys.exit(-1)\n",
    "            # print('step')\n",
    "            self.actionO.put(action)\n",
    "            self.parent.log(('step put action',self.counter))\n",
    "            self.parent.log((self.counter,action))\n",
    "            self.parent.log(('step waiting for reward',self.counter))\n",
    "            reward,done,info=self.rewardI.get()\n",
    "            if not done:\n",
    "                self.parent.log(('step waiting for next state',self.counter))\n",
    "                next_state=self.inputS.get()\n",
    "                self.counter+=1\n",
    "                # self.parent.log((self.counter,next_state))\n",
    "            else: next_state=self.state\n",
    "            # print(self.counter,done)\n",
    "            return next_state,reward,done,info\n",
    "        def print_queues(self):\n",
    "            print('inputS',self.inputS.queue)\n",
    "            print('actionO',self.actionO.queue)\n",
    "            print('rewardI',self.rewardI.queue)\n",
    "            print('outputS',self.outputS.queue)\n",
    "             \n",
    "    class Monitor():\n",
    "        def __init__(self,parent):\n",
    "            self.parent=parent\n",
    "        def run(self,training_steps,N_EPISODES=10):\n",
    "            for episode in range(N_EPISODES):\n",
    "                state=self.parent.env.reset()\n",
    "                for steps in range(training_steps):\n",
    "                    # env.render()\n",
    "                    action=self.parent.env.action_space.sample()\n",
    "                    state, reward, done, info = self.parent.env.step(action)\n",
    "                    self.parent.log(('step returned:',state, reward, done, info))\n",
    "            self.parent.set_training(False)\n",
    "        def train(self,training_steps):\n",
    "            self.parent.actor.model.learn(total_timesteps=training_steps)\n",
    "            self.parent.env.actionO.put(self.parent.env.action_space.sample())\n",
    "            self.parent.set_training(False)\n",
    "            # self.parent.log((self.parent.training,self.parent))\n",
    "    \n",
    "    class Actor(Actor):\n",
    "        def __init__(self,parent,model):\n",
    "            super().__init__(parent=parent,model=model)\n",
    "        def call_model(self,state):\n",
    "        ##Overriding AIAgent\n",
    "            time=self.parent.time\n",
    "            if self.parent.training: \n",
    "                state=self.parent.get_win_state(state)\n",
    "                self.parent.env.inputS.put(state)\n",
    "                self.parent.log(('call model put state at time',time))\n",
    "                self.parent.log(('call model waiting for action at time',time))\n",
    "                try: action = self.parent.env.actionO.get()#timeout=5)\n",
    "                except: action=0\n",
    "                self.parent.log(('call model received action at time',time))\n",
    "            else:\n",
    "                state=self.parent.get_win_state(state)\n",
    "                action, _states = self.model.predict(state)\n",
    "            return action\n",
    "        def augment_state(self,state):\n",
    "            if not self.parent.metarl: return state\n",
    "            if self.parent.time-1 in self.parent.memory.sar_memory:\n",
    "                prev_action=self.parent.memory.sar_memory[self.parent.time-1]['action']\n",
    "                prev_reward=self.parent.memory.sar_memory[self.parent.time-1]['reward']\n",
    "            else:\n",
    "                if self.parent.action_dim==1: prev_action,prev_reward=0,0\n",
    "                else: prev_action,prev_reward=np.zeros(self.parent.action_dim),0\n",
    "            if not is_sequence(prev_action): \n",
    "                state=np.concatenate([state,np.array([prev_action,prev_reward])])\n",
    "            elif is_sequence(prev_action):\n",
    "                state=np.concatenate([state,prev_action,np.array([prev_reward])])\n",
    "            return state\n",
    "    # Win Specific\n",
    "    def get_win_state(self,state):\n",
    "        if self.time-self.win+1>=0:\n",
    "            prev_stateL=[self.memory.sar_memory[self.time-w]['state'] for \n",
    "                         w in range(1,self.win) if self.time>=w]\n",
    "            win_state=np.concatenate([state]+prev_stateL)\n",
    "        else: \n",
    "            prev_stateL=[state]*self.win\n",
    "            win_state=np.concatenate(prev_stateL)\n",
    "        return win_state\n",
    "    ###\n",
    "    def perceive_reward(self,reward):\n",
    "        return reward[0]\n",
    "    def get_intrinsic_reward(self):\n",
    "        M=self.memory.sar_memory\n",
    "        i_rew=0\n",
    "        if self.time-1 in M:\n",
    "            if 'reward' in M[self.time-1]:\n",
    "                i_rew=self.memory.sar_memory[self.time-1]['reward']\n",
    "        else: i_rew=0\n",
    "        return i_rew\n",
    "    def reward(self,reward):\n",
    "        ##Augmenting AIAgent\n",
    "        reward_in=reward\n",
    "        i_rew=self.get_intrinsic_reward()\n",
    "        reward_in=(reward_in[0]+i_rew,reward_in[1],reward_in[2])\n",
    "        reward=super().reward(reward)\n",
    "        self.tot_rew+=reward\n",
    "        if self.training: \n",
    "            self.env.rewardI.put(reward_in)\n",
    "            self.log(('call model put reward at time',self.time))\n",
    "    def begin(self):\n",
    "        ##Augmenting AIAgent\n",
    "        self.rewL+=[self.tot_rew]\n",
    "        super().begin()\n",
    "    def avg_rew(self):\n",
    "        return sum(self.rewL)/len(self.rewL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f041f-78bb-47ed-b616-ed12b6c76658",
   "metadata": {},
   "source": [
    "OLD STUFF can be deleted in due course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17bbb0-57cd-41c6-8158-589ca460497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RLAgent(AIAgent):\n",
    "#     def __init__(self,algoclass,action_space,observation_space,policytype='MlpPolicy',\n",
    "#                 n_steps=2048,verbose=1):\n",
    "#         ##Augmenting AIAgent\n",
    "#         super().__init__()\n",
    "#         ##Local RL environment running in a thread to interact with World via queues\n",
    "#         self.env=self.TrainingEnv(parent=self)\n",
    "#         self.env.action_space=action_space\n",
    "#         self.env.observation_space=observation_space\n",
    "#         self.monitor=self.Monitor(parent=self)\n",
    "#         self.set_training(True)\n",
    "#         self.tot_rew=0\n",
    "#         self.logL=[]\n",
    "#         self.kill=False\n",
    "#         ##Override Actor\n",
    "#         self.model=algoclass(policytype, self.env, verbose=verbose,n_steps=n_steps)\n",
    "#         self.actor=self.Actor(parent=self,model=self.model)\n",
    "#         self.perception.perceive_reward=self.perceive_reward\n",
    "        \n",
    "#     def start(self,training_steps=20000):\n",
    "#         # self.monitorthread=Thread(name='monitor',target=self.monitor.run,args=(training_steps,))\n",
    "#         self.monitorthread=Thread(name='monitor',target=self.monitor.train,args=(training_steps,))\n",
    "#         self.monitorthread.start() \n",
    "    \n",
    "#     def stop(self):\n",
    "#         self.kill=True\n",
    "    \n",
    "#     def log(self,entry):\n",
    "#         self.logL+=[entry]\n",
    "        \n",
    "#     def set_training(self,value):\n",
    "#         self.training=value\n",
    "        \n",
    "#     class TrainingEnv(Env):\n",
    "#         def __init__(self,parent):\n",
    "#             self.parent=parent\n",
    "#             self.inputS=Queue() #written by act read by reset and step\n",
    "#             self.outputS=Queue() #written by act read by act\n",
    "#             self.rewardI=Queue() #written by act and read by step\n",
    "#             self.actionO=Queue() #written by step and read by act\n",
    "#             self.counter=0\n",
    "#         def reset(self):\n",
    "#             # print('reset')\n",
    "#             self.parent.log(('reset waiting',self.counter))\n",
    "#             self.state=self.inputS.get()\n",
    "#             return self.state\n",
    "#         def step(self,action):\n",
    "#             if self.parent.kill: sys.exit(-1)\n",
    "#             # print('step')\n",
    "#             self.actionO.put(action)\n",
    "#             self.parent.log(('step put action',self.counter))\n",
    "#             self.parent.log((self.counter,action))\n",
    "#             self.parent.log(('step waiting for reward',self.counter))\n",
    "#             reward,done,info=self.rewardI.get()\n",
    "#             if not done:\n",
    "#                 self.parent.log(('step waiting for next state',self.counter))\n",
    "#                 next_state=self.inputS.get()\n",
    "#                 self.counter+=1\n",
    "#                 # self.parent.log((self.counter,next_state))\n",
    "#             else: next_state=self.state\n",
    "#             # print(self.counter,done)\n",
    "#             return next_state,reward,done,info\n",
    "#         def print_queues(self):\n",
    "#             print('inputS',self.inputS.queue)\n",
    "#             print('actionO',self.actionO.queue)\n",
    "#             print('rewardI',self.rewardI.queue)\n",
    "#             print('outputS',self.outputS.queue)\n",
    "             \n",
    "#     class Monitor():\n",
    "#         def __init__(self,parent):\n",
    "#             self.parent=parent\n",
    "#         def run(self,training_steps,N_EPISODES=10):\n",
    "#             for episode in range(N_EPISODES):\n",
    "#                 state=self.parent.env.reset()\n",
    "#                 for steps in range(training_steps):\n",
    "#                     # env.render()\n",
    "#                     action=self.parent.env.action_space.sample()\n",
    "#                     state, reward, done, info = self.parent.env.step(action)\n",
    "#                     self.parent.log(('step returned:',state, reward, done, info))\n",
    "#             self.parent.set_training(False)\n",
    "#         def train(self,training_steps):\n",
    "#             self.parent.actor.model.learn(total_timesteps=training_steps)\n",
    "#             self.parent.env.actionO.put(self.parent.env.action_space.sample())\n",
    "#             self.parent.set_training(False)\n",
    "#             # self.parent.log((self.parent.training,self.parent))\n",
    "    \n",
    "#     class Actor(Actor):\n",
    "#         def __init__(self,parent,model):\n",
    "#             super().__init__(parent=parent,model=model)\n",
    "#         def call_model(self,state):\n",
    "#         ##Overriding AIAgent\n",
    "#             time=self.parent.time\n",
    "#             if self.parent.training: \n",
    "#                 self.parent.env.inputS.put(state)\n",
    "#                 self.parent.log(('call model put state at time',time))\n",
    "#                 self.parent.log(('call model waiting for action at time',time))\n",
    "#                 try: action = self.parent.env.actionO.get()#timeout=5)\n",
    "#                 except: action=0\n",
    "#                 self.parent.log(('call model received action at time',time))\n",
    "#             else: action, _states = self.model.predict(state)\n",
    "#             return action\n",
    "        \n",
    "#     def perceive_reward(self,reward):\n",
    "#         return reward[0]\n",
    "#     def reward(self,reward):\n",
    "#         ##Augmenting AIAgent\n",
    "#         reward_in=reward\n",
    "#         reward=super().reward(reward)\n",
    "#         self.tot_rew+=reward\n",
    "#         if self.training: \n",
    "#             self.env.rewardI.put(reward_in)\n",
    "#             self.log(('call model put reward at time',self.time))\n",
    "#     def begin(self):\n",
    "#         ##Augmenting AIAgent\n",
    "#         self.rewL+=[self.tot_rew]\n",
    "#         super().begin()\n",
    "#     def avg_rew(self):\n",
    "#         return sum(self.rewL)/len(self.rewL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803b44e-37ff-494c-b8de-e0fa40a575b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RLAgent(AIAgent):\n",
    "#     def __init__(self,action_space,observation_space,algoclass,training_steps=20000,\n",
    "#                 policytype='MlpPolicy',verbose=0):\n",
    "#         ##Augmenting AIAgent\n",
    "#         super().__init__()\n",
    "#         ##Local RL environment running in a thread to interact with World via queues\n",
    "#         self.env=self.TrainingEnv(parent=self)\n",
    "#         self.env.action_space=action_space\n",
    "#         self.env.observation_space=observation_space\n",
    "#         self.monitor=self.Monitor(parent=self)\n",
    "#         self.set_training(True)\n",
    "#         self.monitorthread=Thread(name='monitor',target=self.monitor.train,args=(training_steps,))\n",
    "#         self.tot_rew=0\n",
    "#         self.logL=[]\n",
    "#         self.kill=False\n",
    "#         ##Override Actor\n",
    "#         self.model=algoclass(policytype, self.env, verbose=verbose,n_steps=training_steps)\n",
    "#         self.actor=self.Actor(parent=self,model=self.model)\n",
    "        \n",
    "#     def start(self):\n",
    "#         self.monitorthread.start() \n",
    "    \n",
    "#     def stop(self):\n",
    "#         self.kill=True\n",
    "    \n",
    "#     def log(self,entry):\n",
    "#         self.logL+=[entry]\n",
    "        \n",
    "#     def set_training(self,value):\n",
    "#         self.training=value\n",
    "        \n",
    "#     class TrainingEnv(Env):\n",
    "#         def __init__(self,parent):\n",
    "#             self.parent=parent\n",
    "#             # self.observation_space=spaces.Box(\n",
    "#             #     low=np.array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38]), \n",
    "#             #     high=np.array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38]), \n",
    "#             #     shape=(4,), dtype=np.float32)\n",
    "#             self.inputS=Queue() #written by act read by reset and step\n",
    "#             self.outputS=Queue() #written by act read by act\n",
    "#             self.rewardI=Queue() #written by act and read by step\n",
    "#             self.actionO=Queue() #written by step and read by act\n",
    "#             self.counter=0\n",
    "#         def reset(self):\n",
    "#             # print('reset')\n",
    "#             self.parent.log(('reset waiting',self.counter))\n",
    "#             self.state=self.inputS.get()\n",
    "#             return self.state\n",
    "#         def step(self,action):\n",
    "#             if self.parent.kill: sys.exit(-1)\n",
    "#             # print('step')\n",
    "#             self.actionO.put(action)\n",
    "#             self.parent.log(('step put action',self.counter))\n",
    "#             self.parent.log((self.counter,action))\n",
    "#             self.parent.log(('step waiting for reward',self.counter))\n",
    "#             reward,done,info=self.rewardI.get()\n",
    "#             if not done:\n",
    "#                 self.parent.log(('step waiting for next state',self.counter))\n",
    "#                 next_state=self.inputS.get()\n",
    "#                 self.counter+=1\n",
    "#                 self.parent.log((self.counter,next_state))\n",
    "#             else: next_state=self.state\n",
    "#             # print(self.counter,done)\n",
    "#             return next_state,reward,done,info\n",
    "#         def print_queues(self):\n",
    "#             print('inputS',self.inputS.queue)\n",
    "#             print('actionO',self.actionO.queue)\n",
    "#             print('rewardI',self.rewardI.queue)\n",
    "#             print('outputS',self.outputS.queue)\n",
    "             \n",
    "#     class Monitor():\n",
    "#         def __init__(self,parent):\n",
    "#             self.parent=parent\n",
    "#         def run(self,training_steps,N_EPISODES=10):\n",
    "#             for episode in range(N_EPISODES):\n",
    "#                 state=self.parent.env.reset()\n",
    "#                 for steps in range(training_steps):\n",
    "#                     # env.render()\n",
    "#                     action=self.parent.env.action_space.sample()\n",
    "#                     state, reward, done, info = self.parent.env.step(action)\n",
    "#                     # print(next_state, reward, done, info, action)\n",
    "#             self.parent.set_training(False)\n",
    "#         def train(self,training_steps):\n",
    "#             self.parent.actor.model.learn(total_timesteps=training_steps)\n",
    "#             self.parent.env.actionO.put(self.parent.env.action_space.sample())\n",
    "#             self.parent.set_training(False)\n",
    "#             # self.parent.log((self.parent.training,self.parent))\n",
    "    \n",
    "#     class Actor(Actor):\n",
    "#         def __init__(self,parent,model):\n",
    "#             super().__init__(parent=parent,model=model)\n",
    "#         def call_model(self,state):\n",
    "#         ##Overriding AIAgent\n",
    "#             time=self.parent.time\n",
    "#             if self.parent.training: \n",
    "#                 self.parent.env.inputS.put(state)\n",
    "#                 self.parent.log(('call model put state at time',time))\n",
    "#                 self.parent.log(('call model waiting for action at time',time))\n",
    "#                 try: action = self.parent.env.actionO.get()#timeout=5)\n",
    "#                 except: action=0\n",
    "#                 self.parent.log(('call model received action at time',time))\n",
    "#             else: action, _states = self.model.predict(state)\n",
    "#             return action\n",
    "#         def compute_reward(self,reward):\n",
    "#             return reward[0]\n",
    "    \n",
    "#     def reward(self,reward):\n",
    "#         ##Augmenting AIAgent\n",
    "#         reward=super().reward(reward)\n",
    "#         self.tot_rew+=reward[0]\n",
    "#         if self.training: \n",
    "#             self.env.rewardI.put(reward)\n",
    "#             self.log(('call model put reward at time',self.time))\n",
    "#     def begin(self):\n",
    "#         ##Augmenting AIAgent\n",
    "#         self.rewL+=[self.tot_rew]\n",
    "#         super().begin()\n",
    "#     def avg_rew(self):\n",
    "#         return sum(self.rewL)/len(self.rewL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8449e81-db30-4246-b5cc-5698ff25765e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
