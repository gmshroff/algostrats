{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b88fd1-07a4-4324-9278-f5accca9776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025c06e6-0084-4c86-8bcd-d8a33adc0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import threading\n",
    "import gym\n",
    "from gym import spaces,Env\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7889ded-dd8c-4c73-aadd-687cf98ddbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from datetime import datetime as dt\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27681588-e85b-4818-b7f9-9e8aca7d87cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from feeds.ipynb\n",
      "importing Jupyter notebook from synfeed.ipynb\n",
      "importing Jupyter notebook from india_calendar.ipynb\n",
      "importing Jupyter notebook from featfuncs.ipynb\n"
     ]
    }
   ],
   "source": [
    "from feeds import BackFeed,DataFeed\n",
    "from featfuncs import feat_aug,add_addl_features_feed,add_ta_features_feed,add_sym_feature_feed\n",
    "from featfuncs import add_global_indices_feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8da69070-9830-44a9-bd5d-0f9800a0c613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from feed_env.ipynb\n"
     ]
    }
   ],
   "source": [
    "from feed_env import Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8234fa-666c-4768-a090-b01237918b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aspectlib\n",
    "import gym\n",
    "from gym import spaces,Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd63887a-5eb4-4b8e-a915-47a864cc08f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from aiagentbase.ipynb\n"
     ]
    }
   ],
   "source": [
    "from aiagentbase import AIAgent,Controller,Memory,Perception,Actor,RLAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65eaa0dd-8ab9-4bdc-b508-1be8997507d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from featfuncs import USE_COLS_DICT_ORIG,update_use_cols_dict\n",
    "GDIM=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afea58b9-6c56-40c8-a101-32cba908483e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OHLCV_COLS=['Open_n','High_n','Low_n','Close_n','Volume_n']\n",
    "# TA_COLS=['SMA_10', 'SMA_20','VOL_SMA_20','RSI_14','BBL_5_2.0','BBM_5_2.0','BBU_5_2.0',\n",
    "#        'BBB_5_2.0', 'BBP_5_2.0','MACD_12_26_9','MACDh_12_26_9','MACDs_12_26_9','VWAP_D',\n",
    "#         'MOM_30','CMO_14']\n",
    "# TA_COLS_MIN=['SMA_10', 'SMA_20','CMO_14']\n",
    "# LOGICAL_FEATURES=['Open_changelen',\n",
    "#  'High_changelen',\n",
    "#  'Low_changelen',\n",
    "#  'Close_changelen',\n",
    "#  'High-Low_changelen',\n",
    "#  'Open-Close_changelen',\n",
    "#  'Open_changelen_val',\n",
    "#  'Open_changelen_polarity',\n",
    "#  'High_changelen_val',\n",
    "#  'High_changelen_polarity',\n",
    "#  'Low_changelen_val',\n",
    "#  'Low_changelen_polarity',\n",
    "#  'Close_changelen_val',\n",
    "#  'Close_changelen_polarity',\n",
    "#  'High-Low_changelen_val',\n",
    "#  'High-Low_changelen_polarity',\n",
    "#  'Open-Close_changelen_val',\n",
    "#  'Open-Close_changelen_polarity']\n",
    "# # COLS=['row_num']+OHLCV_COLS+TA_COLS\n",
    "# # COLS=['row_num']+OHLCV_COLS+TA_COLS_MIN\n",
    "# # DIM=len(COLS)\n",
    "# GDIM=3\n",
    "# MINCOLS=['row_num']+OHLCV_COLS+TA_COLS_MIN\n",
    "# ALLCOLS=['row_num']+OHLCV_COLS+TA_COLS\n",
    "# MINLOG=['row_num']+OHLCV_COLS+TA_COLS_MIN+LOGICAL_FEATURES\n",
    "# ALLLOG=['row_num']+OHLCV_COLS+TA_COLS_MIN+LOGICAL_FEATURES\n",
    "# LOGCOLS=['row_num']+OHLCV_COLS+LOGICAL_FEATURES\n",
    "# USE_COLS_DICT={'allcols':ALLCOLS,'mincols':MINCOLS,'minlog':MINLOG,\n",
    "#            'alllog':ALLLOG,'logcols':LOGCOLS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf1f9c-6a1b-48c4-a162-db735c649a0f",
   "metadata": {},
   "source": [
    "### Trading strategies as agents\n",
    "Interface between backtest and tradeserver. TBD: common rewards format\n",
    "Note: only works for local strategies (not remote: TBD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "358be953-26c6-4e1a-ad66-1a30a5db52f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratAgent(AIAgent):\n",
    "    def __init__(self,use_cols=None):\n",
    "        self.agent=True\n",
    "        self.tidx=0\n",
    "        self.owner=None\n",
    "        super().__init__()\n",
    "        self.use_alt_data=False\n",
    "        self.use_memory=False\n",
    "        # self.actor=self.Actor(parent=self)\n",
    "        self.logL=[]\n",
    "        self.action_space=spaces.Discrete(3)\n",
    "        if use_cols:\n",
    "            USE_COLS_DICT=update_use_cols_dict(USE_COLS_DICT_ORIG)\n",
    "            self.feat_cols=USE_COLS_DICT[use_cols]\n",
    "            self.dim=len(self.feat_cols)\n",
    "            self.data_cols=['datetime']+self.feat_cols\n",
    "        self.model_type='RL'\n",
    "        self.actor.call_model=lambda state: self.call_model(state)\n",
    "        self.actor.percept_to_state=self.percept_to_state\n",
    "    def percept_to_state(self,perceived_state):\n",
    "        # print(perceived_state)\n",
    "        percept=perceived_state['percept']\n",
    "        state=torch.tensor(percept[self.feat_cols].iloc[-1].values)\n",
    "        return state\n",
    "    def initialize(self):\n",
    "        self.agent=True\n",
    "        self.tidx=0\n",
    "        self.owner=None\n",
    "        self.use_memory=False\n",
    "        self.logL=[]\n",
    "        self.action_space=spaces.Discrete(3)\n",
    "        self.data_cols=['datetime']+self.feat_cols\n",
    "        # self.data_cols=['datetime']+COLS\n",
    "        self.model_type='RL'\n",
    "    ##Interface to tradeserver\n",
    "    def compress_alt_data(self,gdata):\n",
    "        gstateD={}\n",
    "        cD={}\n",
    "        dD={}\n",
    "        for d in gdata:\n",
    "            gd=gdata[d][0]\n",
    "            v,m=0,0\n",
    "            for gt in ['^NSEI','^NYA','LSEG.L','^IXIC']:\n",
    "                if 'Open_'+gt in gd:\n",
    "                    cD[gt]=gd['Close_'+gt]-gd['Open_'+gt]\n",
    "                    dD[gt]=gd['High_'+gt]-gd['Low_'+gt]\n",
    "                    v+=gd['Volume_'+gt]\n",
    "                    m+=1\n",
    "                else: cD[gt],dD[gt]=0,0\n",
    "            v=v/m\n",
    "            gstateD[d]=torch.tensor([sum([cD[c] for c in cD])/m,sum([dD[d] for d in dD])/m,v])\n",
    "            gstateD[d]=torch.nan_to_num(gstateD[d])\n",
    "        return gstateD\n",
    "    def set_alt_data(self,alt_data_func,remote=False):\n",
    "        self.use_alt_data=True\n",
    "        if remote: self.gdata=anvil.server.call(alt_data_func)['gdata']\n",
    "        else: self.gdata=alt_data_func()['gdata']\n",
    "        self.gstateD=self.compress_alt_data(self.gdata)\n",
    "        # self.gstateD={d: torch.tensor(pd.DataFrame(self.gdata[d]).iloc[-1].values)\n",
    "        #              for d in self.gdata}\n",
    "    def act_on_entry(self):\n",
    "        if self.owner==None: return True\n",
    "        elif self.owner.status[self.scantickers[self.tidx]]=='deployed': return False\n",
    "        else: return True\n",
    "    def act_on_exit(self):\n",
    "        if self.owner==None: return True\n",
    "        elif self.owner.status[self.scantickers[self.tidx]]=='active': return False\n",
    "        else: return True\n",
    "    def check_entry_batch(self,dfD):\n",
    "        decisionsD,stopD,targetD=self.check_entry(dfD)\n",
    "        if self.act_on_entry(): \n",
    "            trt=self.scantickers[self.tidx]\n",
    "            df=dfD[trt]\n",
    "            action=self.act(df)\n",
    "            decisionsD[trt]=action-1\n",
    "        return decisionsD,stopD,targetD\n",
    "    def save_func(self,episode_state):\n",
    "        return ()\n",
    "    def check_exit_batch(self,dfD,posf):\n",
    "        return posf\n",
    "    def exit_predicate(self,row,df):\n",
    "        return False\n",
    "    def Check(strat,dfD):\n",
    "        return strat.check_entry_batch(dfD)\n",
    "    def Exit(strat,dfD,posf):\n",
    "        return strat.check_exit_batch(dfD,posf)\n",
    "    def check_entry(self,dfD):\n",
    "        decisionsD={t:0 for t in dfD}\n",
    "        stopD={t:0 for t in dfD}\n",
    "        targetD={t:0 for t in dfD}\n",
    "        return decisionsD,stopD,targetD\n",
    "    def exit_func(self,row,df):\n",
    "        return False\n",
    "    def save_func(self,episode_state):\n",
    "        ticker=[t for t in episode_state][0]\n",
    "        return ticker,self.entry_val[ticker],self.exit_val[ticker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52934c55-8b6b-48ab-91cb-f01a606844cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomStratAgent(StratAgent):\n",
    "    # Adaptive momentum strategy using CMO and ADF for non-stationarity\n",
    "    def __init__(self):\n",
    "        # super().__init__()\n",
    "        StratAgent.__init__(self)\n",
    "        self.logL=[]\n",
    "        self.logL=[]\n",
    "        self.rewL=[]\n",
    "        ## Augmentations of AIAgent\n",
    "        self.perception.perceive_reward=self.perceive_reward\n",
    "    def call_model(self,state):\n",
    "        # super().act(state)\n",
    "        return self.action_space.sample()\n",
    "    def perceive_reward(self,reward):\n",
    "        return reward[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f37c9-c85d-40de-863a-3a29a2adb9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLStratAgentDyn(RLAgent,StratAgent):\n",
    "    def __init__(self,algoclass,soclass,monclass,myargs=(2048,False,5,'mincols'),verbose=1,\n",
    "                 metarl=True):\n",
    "        use_cols=myargs[3]\n",
    "        action_space=spaces.MultiDiscrete([3,3,3])\n",
    "        n_steps,self.use_alt_data,win=myargs[0],myargs[1],myargs[2]\n",
    "        USE_COLS_DICT=update_use_cols_dict(USE_COLS_DICT_ORIG)\n",
    "        self.feat_cols=USE_COLS_DICT[use_cols]\n",
    "        self.dim=len(self.feat_cols)\n",
    "        self.data_cols=['datetime']+self.feat_cols\n",
    "        if self.use_alt_data:dim=self.dim+GDIM\n",
    "        else:dim=self.dim\n",
    "        observation_space=spaces.Box(high=np.inf*np.ones(dim),low=-np.inf*np.ones(dim))\n",
    "        super().__init__(algoclass,monclass=monclass,action_space=action_space,observation_space=observation_space,\n",
    "                        n_steps=n_steps,verbose=verbose,win=win,soclass=soclass,metarl=metarl)\n",
    "        self.actor.percept_to_state=self.percept_to_state\n",
    "        # self.perception.perceive_reward=self.perceive_reward\n",
    "    def load_model(self,filepath='./saved_models/PPO0.pth'):\n",
    "        self.model.policy.load_state_dict(torch.load(filepath))\n",
    "    def percept_to_state(self,perceived_state):\n",
    "        # self.logL+=[{'printing perceived_state':perceived_state}]\n",
    "        percept=perceived_state['percept']\n",
    "        if self.use_alt_data:\n",
    "            dstate=torch.tensor(percept[0][self.feat_cols].iloc[-1].values)\n",
    "            gstate=percept[1]\n",
    "            state=torch.concat((dstate,gstate))\n",
    "        else: state=torch.tensor(percept[self.feat_cols].iloc[-1].values)\n",
    "        return state\n",
    "    def clear_sar_memory(self):\n",
    "        if len(self.memory.sar_memory)==0: return\n",
    "        t0=max(self.time-1,0)\n",
    "        zero_state=np.zeros(self.memory.sar_memory[0]['state'].shape[0])\n",
    "        for w in range(self.win):\n",
    "            if self.time-w in self.memory.sar_memory:\n",
    "                self.memory.sar_memory[self.time-w]['state']=zero_state\n",
    "                self.memory.sar_memory[self.time-w]['action']=[0,0,0]\n",
    "                self.memory.sar_memory[self.time-w]['reward']=0\n",
    "    def check_entry_batch(self,dfD):\n",
    "        if self.use_alt_data: \n",
    "            gfD={t:self.gstateD[dfD[t].iloc[-1]['Date']] if dfD[t].iloc[-1]['Date'] \n",
    "                 in self.gstateD else torch.zeros(3) for t in dfD}\n",
    "        decisionsD,stopD,targetD={},{},{}\n",
    "        if self.act_on_entry(): \n",
    "            limit={0:.5,1:1,2:2}\n",
    "            trt=self.scantickers[self.tidx]\n",
    "            df=dfD[trt]\n",
    "            if self.use_alt_data: action=self.act((df,gfD[trt]))\n",
    "            else: action=self.act(df)\n",
    "            decisionsD[trt]=action[0]-1\n",
    "            stopD[trt]=limit[action[1]]\n",
    "            targetD[trt]=limit[action[2]]\n",
    "        return decisionsD,stopD,targetD\n",
    "    def reward(self,reward):\n",
    "        if type(reward[0])==list: \n",
    "            if len(reward[0])==0: rew=0\n",
    "            else: rew=reward[0][0]['ppnl']\n",
    "            reward=(rew,reward[1],reward[2])\n",
    "        return super().reward(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e90844-5b84-45c2-a37a-9d8741cc98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing(dfD):\n",
    "    empty={t:0 for t in dfD}\n",
    "    return empty,empty,empty\n",
    "def always_buy(dfD):\n",
    "    buy={t:1 for t in dfD}\n",
    "    empty={t:0 for t in dfD}\n",
    "    return buy,empty,empty\n",
    "def always_sell(dfD):\n",
    "    sell={t:-1 for t in dfD}\n",
    "    empty={t:0 for t in dfD}\n",
    "    return sell,empty,empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcef68c-7c7f-4309-bb90-0c155890a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alt_data():\n",
    "    global feed\n",
    "    aD={'gdata':feed.gdata}\n",
    "    return aD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607ba7b-13da-43d3-857c-16f5e832fed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
