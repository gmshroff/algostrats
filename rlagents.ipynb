{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b88fd1-07a4-4324-9278-f5accca9776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c06e6-0084-4c86-8bcd-d8a33adc0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import threading\n",
    "import gym\n",
    "from gym import spaces,Env\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7889ded-dd8c-4c73-aadd-687cf98ddbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from datetime import datetime as dt\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27681588-e85b-4818-b7f9-9e8aca7d87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feeds import BackFeed,DataFeed\n",
    "from featfuncs import feat_aug,add_addl_features_feed,add_ta_features_feed,add_sym_feature_feed\n",
    "from featfuncs import add_global_indices_feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da69070-9830-44a9-bd5d-0f9800a0c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feed_env import Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8234fa-666c-4768-a090-b01237918b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aspectlib\n",
    "import gym\n",
    "from gym import spaces,Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd63887a-5eb4-4b8e-a915-47a864cc08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiagentbase import AIAgent,Controller,Memory,Perception,Actor,RLAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afea58b9-6c56-40c8-a101-32cba908483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OHLCV_COLS=['Open_n','High_n','Low_n','Close_n','Volume_n']\n",
    "TA_COLS=['SMA_10', 'SMA_20','VOL_SMA_20','RSI_14','BBL_5_2.0','BBM_5_2.0','BBU_5_2.0',\n",
    "       'BBB_5_2.0', 'BBP_5_2.0','MACD_12_26_9','MACDh_12_26_9','MACDs_12_26_9','VWAP_D',\n",
    "        'MOM_30','CMO_14']\n",
    "TA_COLS_MIN=['SMA_10', 'SMA_20','CMO_14']\n",
    "# COLS=['row_num']+OHLCV_COLS+TA_COLS\n",
    "COLS=['row_num']+OHLCV_COLS+TA_COLS_MIN\n",
    "DIM=len(COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf1f9c-6a1b-48c4-a162-db735c649a0f",
   "metadata": {},
   "source": [
    "### Trading strategies as agents\n",
    "Interface between backtest and tradeserver. TBD: common rewards format\n",
    "Note: only works for local strategies (not remote: TBD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358be953-26c6-4e1a-ad66-1a30a5db52f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratAgent(AIAgent):\n",
    "    def __init__(self):\n",
    "        self.agent=True\n",
    "        self.tidx=0\n",
    "        self.owner=None\n",
    "        super().__init__()\n",
    "        self.use_memory=False\n",
    "        # self.actor=self.Actor(parent=self)\n",
    "        self.logL=[]\n",
    "        self.action_space=spaces.Discrete(3)\n",
    "        self.data_cols=['datetime']+COLS\n",
    "        self.model_type='RL'\n",
    "        self.actor.call_model=lambda state: self.call_model(state)\n",
    "        self.actor.percept_to_state=self.percept_to_state\n",
    "    def percept_to_state(self,perceived_state):\n",
    "        # print(perceived_state)\n",
    "        percept=perceived_state['percept']\n",
    "        state=torch.tensor(percept[COLS].iloc[-1].values)\n",
    "        return state\n",
    "    def initialize(self):\n",
    "        self.agent=True\n",
    "        self.tidx=0\n",
    "        self.owner=None\n",
    "        self.use_memory=False\n",
    "        self.logL=[]\n",
    "        self.action_space=spaces.Discrete(3)\n",
    "        self.data_cols=['datetime']+COLS\n",
    "        self.model_type='RL'\n",
    "    ##Interface to tradeserver\n",
    "    def set_alt_data(self,alt_data_func,remote=False):\n",
    "        if remote: self.gdata=anvil.server.call(alt_data_func)['gdata']\n",
    "        else: self.gdata=alt_data_func()['gdata']\n",
    "    def act_on_entry(self):\n",
    "        if self.owner==None: return True\n",
    "        elif self.owner.status[self.scantickers[self.tidx]]=='deployed': return False\n",
    "        else: return True\n",
    "    def act_on_exit(self):\n",
    "        if self.owner==None: return True\n",
    "        elif self.owner.status[self.scantickers[self.tidx]]=='active': return False\n",
    "        else: return True\n",
    "    def check_entry_batch(self,dfD):\n",
    "        decisionsD,stopD,targetD=self.check_entry(dfD)\n",
    "        if self.act_on_entry(): \n",
    "            trt=self.scantickers[self.tidx]\n",
    "            df=dfD[trt]\n",
    "            action=self.act(df)\n",
    "            decisionsD[trt]=action-1\n",
    "        return decisionsD,stopD,targetD\n",
    "    def save_func(self,episode_state):\n",
    "        return ()\n",
    "    def check_exit_batch(self,dfD,posf):\n",
    "        return posf\n",
    "    def exit_predicate(self,row,df):\n",
    "        return False\n",
    "    def Check(strat,dfD):\n",
    "        return strat.check_entry_batch(dfD)\n",
    "    def Exit(strat,dfD,posf):\n",
    "        return strat.check_exit_batch(dfD,posf)\n",
    "    def check_entry(self,dfD):\n",
    "        decisionsD={t:0 for t in dfD}\n",
    "        stopD={t:0 for t in dfD}\n",
    "        targetD={t:0 for t in dfD}\n",
    "        return decisionsD,stopD,targetD\n",
    "    def exit_func(self,row,df):\n",
    "        return False\n",
    "    def save_func(self,episode_state):\n",
    "        ticker=[t for t in episode_state][0]\n",
    "        return ticker,self.entry_val[ticker],self.exit_val[ticker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52934c55-8b6b-48ab-91cb-f01a606844cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomStratAgent(StratAgent):\n",
    "    # Adaptive momentum strategy using CMO and ADF for non-stationarity\n",
    "    def __init__(self):\n",
    "        # super().__init__()\n",
    "        StratAgent.__init__(self)\n",
    "        self.logL=[]\n",
    "        self.logL=[]\n",
    "        self.rewL=[]\n",
    "        ## Augmentations of AIAgent\n",
    "        self.perception.perceive_reward=self.perceive_reward\n",
    "    def call_model(self,state):\n",
    "        # super().act(state)\n",
    "        return self.action_space.sample()\n",
    "    def perceive_reward(self,reward):\n",
    "        return reward[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f37c9-c85d-40de-863a-3a29a2adb9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLStratAgentDyn(RLAgent,StratAgent):\n",
    "    def __init__(self,algoclass,soclass,monclass,n_steps=2048,dim=DIM,verbose=1,win=5,metarl=True):\n",
    "        action_space=spaces.MultiDiscrete([3,3,3])\n",
    "        observation_space=spaces.Box(high=np.inf*np.ones(dim),low=-np.inf*np.ones(dim))\n",
    "        super().__init__(algoclass,monclass=monclass,action_space=action_space,observation_space=observation_space,\n",
    "                        n_steps=n_steps,verbose=verbose,win=win,soclass=soclass,metarl=metarl)\n",
    "        self.actor.percept_to_state=self.percept_to_state\n",
    "        # self.perception.perceive_reward=self.perceive_reward\n",
    "    def load_model(self,filepath='./saved_models/PPO0.pth'):\n",
    "        self.model.policy.load_state_dict(torch.load(filepath))\n",
    "    def percept_to_state(self,perceived_state):\n",
    "        # self.logL+=[{'printing perceived_state':perceived_state}]\n",
    "        percept=perceived_state['percept']\n",
    "        state=torch.tensor(percept[COLS].iloc[-1].values)\n",
    "        return state\n",
    "    def clear_sar_memory(self):\n",
    "        if len(self.memory.sar_memory)==0: return\n",
    "        t0=max(self.time-1,0)\n",
    "        zero_state=np.zeros(self.memory.sar_memory[0]['state'].shape[0])\n",
    "        for w in range(self.win):\n",
    "            if self.time-w in self.memory.sar_memory:\n",
    "                self.memory.sar_memory[self.time-w]['state']=zero_state\n",
    "                self.memory.sar_memory[self.time-w]['action']=[0,0,0]\n",
    "                self.memory.sar_memory[self.time-w]['reward']=0\n",
    "    def check_entry_batch(self,dfD):\n",
    "        decisionsD,stopD,targetD={},{},{}\n",
    "        if self.act_on_entry(): \n",
    "            limit={0:.5,1:1,2:2}\n",
    "            trt=self.scantickers[self.tidx]\n",
    "            df=dfD[trt]\n",
    "            action=self.act(df)\n",
    "            decisionsD[trt]=action[0]-1\n",
    "            stopD[trt]=limit[action[1]]\n",
    "            targetD[trt]=limit[action[2]]\n",
    "        return decisionsD,stopD,targetD\n",
    "    # def check_entry(self,dfD):\n",
    "    #     limit={0:.5,1:1,2:2}\n",
    "    #     decisionsD={t:0 for t in dfD}\n",
    "    #     stopD={t:0 for t in dfD}\n",
    "    #     targetD={t:0 for t in dfD}\n",
    "    #     statesD={t:self.percept_to_state({'percept':dfD[t]}) for t in dfD}\n",
    "    #     for t in dfD:\n",
    "    #         state=statesD[t]\n",
    "    #         if self.win>1: state=self.actor.augment_state(state)\n",
    "    #         state=self.get_win_state(state)\n",
    "    #         pred,_=self.model.predict(state)\n",
    "    #         decisionsD[t]=pred[0]-1\n",
    "    #         stopD[t]=pred[1]\n",
    "    #         targetD[t]=pred[2]\n",
    "    #     return decisionsD,stopD,targetD\n",
    "    def reward(self,reward):\n",
    "        if type(reward[0])==list: \n",
    "            if len(reward[0])==0: rew=0\n",
    "            else: rew=reward[0][0]['ppnl']\n",
    "            reward=(rew,reward[1],reward[2])\n",
    "        return super().reward(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0589384-2eb7-4979-b5bc-71fce38e5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLD - CAN RETAIN ONLY ABOVE NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a8c3a-5ecc-469b-9445-951647dda176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RLStratAgent(RLAgent,StratAgent):\n",
    "#     def __init__(self,algoclass,n_steps=2048,dim=DIM,verbose=1,win=1,soclass=None,metarl=False):\n",
    "#         action_space=spaces.Discrete(3)\n",
    "#         observation_space=spaces.Box(high=np.inf*np.ones(dim),low=-np.inf*np.ones(dim))\n",
    "#         super().__init__(algoclass,action_space=action_space,observation_space=observation_space,\n",
    "#                         n_steps=n_steps,verbose=verbose,win=win,soclass=soclass,metarl=metarl)\n",
    "#         self.actor.percept_to_state=self.percept_to_state\n",
    "#         # self.perception.perceive_reward=self.perceive_reward\n",
    "#     def load_model(self,filepath='./saved_models/PPO0.pth'):\n",
    "#         self.model.policy.load_state_dict(torch.load(filepath))\n",
    "#     def percept_to_state(self,perceived_state):\n",
    "#         # self.logL+=[{'printing perceived_state':perceived_state}]\n",
    "#         percept=perceived_state['percept']\n",
    "#         state=torch.tensor(percept[COLS].iloc[-1].values)\n",
    "#         return state\n",
    "#     # def perceive_reward(self,reward):\n",
    "#     #     return reward[0]\n",
    "#     def check_entry(self,dfD):\n",
    "#         decisionsD={t:0 for t in dfD}\n",
    "#         stopD={t:0 for t in dfD}\n",
    "#         targetD={t:0 for t in dfD}\n",
    "#         statesD={t:self.percept_to_state({'percept':dfD[t]}) for t in dfD}\n",
    "#         if self.win==1:decisionsD={t:self.model.predict(statesD[t])[0]-1 for t in dfD}\n",
    "#         else:\n",
    "#             for t in dfD:\n",
    "#                 state=statesD[t]\n",
    "#                 state=self.actor.augment_state(state)\n",
    "#                 state=self.get_win_state(state)\n",
    "#                 decisionsD[t]=self.model.predict(state)[0]-1\n",
    "#         return decisionsD,stopD,targetD\n",
    "#     def reward(self,reward):\n",
    "#         if type(reward[0])==list: \n",
    "#             if len(reward[0])==0: rew=0\n",
    "#             else: rew=reward[0][0]['ppnl']\n",
    "#             reward=(rew,reward[1],reward[2])\n",
    "#         return super().reward(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e90844-5b84-45c2-a37a-9d8741cc98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing(dfD):\n",
    "    empty={t:0 for t in dfD}\n",
    "    return empty,empty,empty\n",
    "def always_buy(dfD):\n",
    "    buy={t:1 for t in dfD}\n",
    "    empty={t:0 for t in dfD}\n",
    "    return buy,empty,empty\n",
    "def always_sell(dfD):\n",
    "    sell={t:-1 for t in dfD}\n",
    "    empty={t:0 for t in dfD}\n",
    "    return sell,empty,empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcef68c-7c7f-4309-bb90-0c155890a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alt_data():\n",
    "    global feed\n",
    "    aD={'gdata':feed.gdata}\n",
    "    return aD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607ba7b-13da-43d3-857c-16f5e832fed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
